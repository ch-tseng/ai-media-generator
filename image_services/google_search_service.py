import os
import requests
import time
from typing import Dict, List, Optional
from urllib.parse import urlparse, parse_qs
import uuid
from pathlib import Path

class GoogleImageSearchService:
    """Google åœ–ç‰‡æœå°‹æœå‹™"""
    
    def __init__(self):
        self.api_key = os.environ.get('GOOGLE_SEARCH_API_KEY')
        self.search_engine_id = os.environ.get('GOOGLE_SEARCH_ENGINE_ID')
        self.base_url = 'https://www.googleapis.com/customsearch/v1'
        self.download_dir = Path('static/downloaded_images')
        self.download_dir.mkdir(exist_ok=True)
        
        # å¦‚æœæ²’æœ‰APIé‡‘é‘°ï¼Œä½¿ç”¨web scrapingæ¨¡å¼
        self.use_api = bool(self.api_key and self.search_engine_id)
        
        if self.use_api:
            print(f"âœ… Google åœ–ç‰‡æœå°‹æœå‹™å·²åˆå§‹åŒ– (APIæ¨¡å¼)")
        else:
            print(f"âœ… Google åœ–ç‰‡æœå°‹æœå‹™å·²åˆå§‹åŒ– (Web Scrapingæ¨¡å¼)")
    
    def search_images(self, 
                     query: str,
                     page: int = 1,
                     per_page: int = 12,
                     **kwargs) -> Dict:
        """
        æœå°‹Googleåœ–ç‰‡
        
        Args:
            query: æœå°‹é—œéµå­—
            page: é ç¢¼
            per_page: æ¯é æ•¸é‡
        """
        if self.use_api:
            print(f"ğŸ” å˜—è©¦ä½¿ç”¨Google Search APIæœå°‹: {query}")
            result = self._search_with_api(query, page, per_page, **kwargs)
            
            # å¦‚æœAPIå¤±æ•—ï¼Œè‡ªå‹•å›é€€åˆ°Web Scraping
            if not result.get('success'):
                print(f"âš ï¸ APIæœå°‹å¤±æ•—ï¼Œå›é€€åˆ°Web Scrapingæ¨¡å¼")
                print(f"âŒ APIéŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                return self._search_with_scraping(query, page, per_page, **kwargs)
            else:
                print(f"âœ… APIæœå°‹æˆåŠŸï¼Œæ‰¾åˆ° {len(result.get('images', []))} å¼µåœ–ç‰‡")
                return result
        else:
            print(f"ğŸ” ä½¿ç”¨Web Scrapingæ¨¡å¼æœå°‹: {query}")
            return self._search_with_scraping(query, page, per_page, **kwargs)
    
    def _search_with_api(self, query: str, page: int, per_page: int, **kwargs) -> Dict:
        """ä½¿ç”¨Google Custom Search APIæœå°‹"""
        try:
            # Google Custom Search APIåƒæ•¸
            start_index = (page - 1) * per_page + 1
            
            params = {
                'key': self.api_key,
                'cx': self.search_engine_id,
                'q': query,
                'searchType': 'image',
                'start': start_index,
                'num': min(per_page, 10),  # APIé™åˆ¶æœ€å¤š10å€‹çµæœ
                'safe': 'active',
                'imgSize': kwargs.get('size', 'medium'),
                'imgType': kwargs.get('type', 'photo'),
                'fileType': 'jpg,png,webp',
                'fields': 'items(title,link,snippet,image/thumbnailLink,image/width,image/height,image/contextLink,fileFormat,displayLink)'  # åªè«‹æ±‚éœ€è¦çš„æ¬„ä½
            }
            
            # æ·»åŠ åœ–ç‰‡å°ºå¯¸éæ¿¾
            if kwargs.get('orientation'):
                if kwargs['orientation'] == 'landscape':
                    params['imgSize'] = 'large'
                elif kwargs['orientation'] == 'portrait':
                    params['imgSize'] = 'medium'
            
            # æ·»åŠ gzipæ”¯æ´çš„headers
            api_headers = {
                'Accept-Encoding': 'gzip',
                'User-Agent': 'ai-media-generator (gzip)'
            }
            
            response = requests.get(self.base_url, params=params, headers=api_headers, timeout=120)
            response.raise_for_status()
            
            data = response.json()
            
            # è™•ç†æœå°‹çµæœ
            results = []
            items = data.get('items', [])
            
            for item in items:
                image_info = {
                    'id': item.get('title', '').replace(' ', '_') + f"_{int(time.time())}",
                    'url': item.get('link'),
                    'thumb_url': item.get('image', {}).get('thumbnailLink'),
                    'title': item.get('title', ''),
                    'description': item.get('snippet', ''),
                    'width': item.get('image', {}).get('width'),
                    'height': item.get('image', {}).get('height'),
                    'source_url': item.get('image', {}).get('contextLink'),
                    'file_type': item.get('fileFormat', '').upper(),
                    'platform': 'google',
                    'attribution': f"ä¾†æº: {urlparse(item.get('displayLink', '')).netloc}",
                    'download_url': item.get('link')
                }
                results.append(image_info)
            
            return {
                'success': True,
                'query': query,
                'page': page,
                'per_page': per_page,
                'total_results': data.get('searchInformation', {}).get('totalResults', 0),
                'results': results,
                'images': results,  # ç‚ºå‰ç«¯å…¼å®¹æ€§æ·»åŠ 
                'platform': 'google',
                'message': f'æ‰¾åˆ° {len(results)} å¼µåœ–ç‰‡'
            }
            
        except requests.exceptions.RequestException as e:
            return {
                'success': False,
                'error': f'APIè«‹æ±‚å¤±æ•—: {str(e)}',
                'message': 'ç„¡æ³•é€£æ¥åˆ°Googleæœå°‹API',
                'results': []
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'æœå°‹å¤±æ•—: {str(e)}',
                'message': 'æœå°‹éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤',
                'results': []
            }
    
    def _search_with_scraping(self, query: str, page: int, per_page: int, **kwargs) -> Dict:
        """ä½¿ç”¨Web Scrapingæœå°‹ï¼ˆç•¶æ²’æœ‰APIé‡‘é‘°æ™‚çš„å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            import re
            import json
            from urllib.parse import quote, unquote
            
            print(f"ğŸ” Web Scrapingæœå°‹: {query}")
            
            # æ§‹å»ºGoogleåœ–ç‰‡æœå°‹URL
            encoded_query = quote(query, safe='')
            start_index = (page - 1) * per_page
            
            # æ§‹å»ºæœå°‹åƒæ•¸
            tbs_params = []
            
            # åœ–ç‰‡å°ºå¯¸åƒæ•¸
            size = kwargs.get('size', 'any')
            if size == 'large':
                tbs_params.append('isz:l')
            elif size == 'medium':
                tbs_params.append('isz:m')
            elif size == 'icon':
                tbs_params.append('isz:i')
            
            # åœ–ç‰‡æ–¹å‘åƒæ•¸
            orientation = kwargs.get('orientation', 'any')
            if orientation == 'landscape':
                tbs_params.append('iar:w')
            elif orientation == 'portrait':
                tbs_params.append('iar:t')
            
            # åœ–ç‰‡é¡å‹åƒæ•¸
            image_type = kwargs.get('type', 'any')
            if image_type == 'photo':
                tbs_params.append('itp:photo')
            elif image_type == 'clipart':
                tbs_params.append('itp:clipart')
            elif image_type == 'lineart':
                tbs_params.append('itp:lineart')
            elif image_type == 'face':
                tbs_params.append('itp:face')
            
            # çµ„åˆtbsåƒæ•¸
            tbs_param = ''
            if tbs_params:
                tbs_param = '&tbs=' + ','.join(tbs_params)
            
            # æ·»åŠ èªè¨€åƒæ•¸ä»¥æ”¯æ´ä¸­æ–‡æœå°‹
            url = f"https://www.google.com/search?q={encoded_query}&tbm=isch&start={start_index}{tbs_param}&hl=zh-TW&gl=TW&safe=off"
            
            headers = {
                'User-Agent': 'ai-media-generator (gzip)',  # åŠ å…¥gzipæ¨™è­˜
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',  # å•Ÿç”¨gzipå£“ç¸®
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            print(f"ğŸ“¡ è«‹æ±‚URL: {url}")
            
            response = requests.get(url, headers=headers, timeout=120)
            response.raise_for_status()
            
            html_content = response.text
            print(f"ğŸ“„ å›æ‡‰é•·åº¦: {len(html_content)} å­—å…ƒ")
            
            results = []
            found_urls = set()  # åˆå§‹åŒ–found_urlsè®Šæ•¸
            
            # æ–¹æ³•1: 2025å¹´æ–°çš„JSONæ•¸æ“šæå–æ–¹æ³•
            # Googleç¾åœ¨ä½¿ç”¨ä¸åŒçš„JSONçµæ§‹å­˜å„²åœ–ç‰‡æ•¸æ“š
            json_patterns = [
                r'AF_initDataCallback\({[^}]*?"ds:1"[^}]*?},{.*?}\);',
                r'window\._sharedData\s*=\s*({.*?});',
                r'AF_initDataCallback\({[^}]*?"ds:0"[^}]*?},{.*?}\);',
                r'\\x22(https?://[^\\]*\.(?:jpg|jpeg|png|webp|gif)[^\\]*?)\\x22'
            ]
            
            for pattern in json_patterns:
                json_matches = re.findall(pattern, html_content, re.DOTALL)
                print(f"  ğŸ” JSONæ¨¡å¼åŒ¹é…åˆ° {len(json_matches)} å€‹çµæœ")
                
                for json_match in json_matches:
                    try:
                        if json_match.startswith('http'):
                            # ç›´æ¥æ˜¯URL
                            if json_match not in found_urls:
                                found_urls.add(json_match)
                        else:
                            # å˜—è©¦å¾JSONä¸­æå–URL
                            url_matches = re.findall(r'https?://[^"\'\\]*\.(?:jpg|jpeg|png|webp|gif)[^"\'\\]*', json_match)
                            for url in url_matches:
                                if url not in found_urls and len(found_urls) < per_page * 3:
                                    found_urls.add(url)
                    except Exception as e:
                        continue
                
                if len(found_urls) >= per_page:
                    print(f"  âœ… JSONè§£æå·²æ‰¾åˆ°è¶³å¤ URL: {len(found_urls)}")
                    break
            
            # æ–¹æ³•2: åŸºæ–¼å¯¦éš›èª¿è©¦çµæœçš„åœ–ç‰‡URLæå–
            # æ ¹æ“šå¯¦éš›çš„Googleé é¢çµæ§‹èª¿æ•´è§£æç­–ç•¥
            img_patterns = [
                # éæ¿¾æ‰Googleè‡ªå·±çš„åœ–ç¤ºå’Œç„¡æ•ˆURL
                r'"(https?://(?!ssl\.gstatic\.com|www\.gstatic\.com|fonts\.gstatic\.com)[^"]*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"]*)?)"',
                # imgæ¨™ç±¤ä¸­çš„srcå±¬æ€§ï¼ˆæ’é™¤Googleè‡ªå·±çš„åœ–ç‰‡ï¼‰
                r'<img[^>]+src="(https?://(?!ssl\.gstatic\.com|www\.gstatic\.com)[^"]*\.(?:jpg|jpeg|png|webp|gif)[^"]*)"',
                # data-srcå±¬æ€§
                r'data-src="(https?://(?!ssl\.gstatic\.com|www\.gstatic\.com)[^"]*\.(?:jpg|jpeg|png|webp|gif)[^"]*)"',
                # é€šç”¨åœ–ç‰‡URLï¼ˆä½†è¦éæ¿¾æ‰æ˜é¡¯çš„ç„¡æ•ˆURLï¼‰
                r'(https?://(?!ssl\.gstatic\.com|www\.gstatic\.com|fonts\.gstatic\.com|www\.png)[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^\s<>"\']*)?)',
                # Googleä»£ç†çš„åœ–ç‰‡URL
                r'"(https://encrypted-tbn\d\.gstatic\.com/images\?[^"]*)"',
                # å…¶ä»–Googleåœ–ç‰‡æœå‹™URL
                r'"(https://lh\d+\.googleusercontent\.com/[^"]*)"',
                # æ›´åš´æ ¼çš„æ¨¡å¼ï¼šåªåŒ¹é…çœ‹èµ·ä¾†åƒçœŸå¯¦åœ–ç‰‡ç¶²ç«™çš„URL
                r'"(https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/[^"]*\.(?:jpg|jpeg|png|webp|gif)(?:\?[^"]*)?)"',
                # JavaScriptä¸­çš„åœ–ç‰‡URL
                r'imgUrl["\']?\s*[:=]\s*["\']([^"\']*\.(?:jpg|jpeg|png|webp|gif)[^"\']*)["\']',
                # å°‹æ‰¾åŒ…å«å¯¦éš›åœ–ç‰‡ç¶²ç«™åŸŸåçš„URL
                r'"(https?://(?:media\.|images\.|cdn\.|static\.|img\.|photo\.)[^"]*\.(?:jpg|jpeg|png|webp|gif)[^"]*)"',
                # Base64ç·¨ç¢¼çš„åœ–ç‰‡æ•¸æ“š
                r'"(data:image/[^;]+;base64,[A-Za-z0-9+/=]{200,})"'
            ]
            
            # found_urlså·²ç¶“åœ¨ä¸Šé¢åˆå§‹åŒ–äº†
            
            print(f"ğŸ” é–‹å§‹æå–åœ–ç‰‡URLï¼Œç›®æ¨™æ•¸é‡: {per_page * 3}")  # å¤šæå–ä¸€äº›
            
            for i, pattern in enumerate(img_patterns):
                print(f"  ğŸ” ä½¿ç”¨æ¨¡å¼ {i+1}: æ­£åœ¨æœå°‹...")
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                new_matches = 0
                for match in matches:
                    if match not in found_urls and len(found_urls) < per_page * 3:
                        found_urls.add(match)
                        new_matches += 1
                print(f"  âœ… æ¨¡å¼ {i+1} æ‰¾åˆ° {len(matches)} å€‹åŒ¹é…ï¼Œæ–°å¢ {new_matches} å€‹URL")
                
                # å¦‚æœå·²ç¶“æ‰¾åˆ°è¶³å¤ çš„URLï¼Œå¯ä»¥æå‰åœæ­¢
                if len(found_urls) >= per_page * 2:
                    print(f"  ğŸ¯ å·²æ‰¾åˆ°è¶³å¤ çš„URL ({len(found_urls)}å€‹)ï¼Œåœæ­¢æœå°‹")
                    break
            
            print(f"ğŸ” ç¸½å…±æ‰¾åˆ° {len(found_urls)} å€‹å”¯ä¸€åœ–ç‰‡URL")
            
            # æ–¹æ³•3: å¦‚æœå‰é¢çš„æ–¹æ³•éƒ½å¤±æ•—ï¼Œä½¿ç”¨æ›´å¯¬é¬†çš„æœå°‹
            if len(found_urls) == 0:
                print("âš ï¸ å‰é¢çš„æ–¹æ³•éƒ½æ²’æ‰¾åˆ°URLï¼Œå˜—è©¦æ›´å¯¬é¬†çš„æœå°‹...")
                
                # æœå°‹æ‰€æœ‰çœ‹èµ·ä¾†åƒåœ–ç‰‡URLçš„å­—ä¸²ï¼ˆæ›´å¯¬é¬†ä½†ä»è¦éæ¿¾ç„¡æ•ˆURLï¼‰
                loose_patterns = [
                    # åŸºæœ¬åœ–ç‰‡URLä½†æ’é™¤æ˜é¡¯çš„ç„¡æ•ˆåŸŸå
                    r'(https?://(?!ssl\.gstatic\.com|www\.gstatic\.com|fonts\.gstatic\.com|www\.png)[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                    # åŒ…å«imagesé—œéµå­—çš„URL
                    r'(https?://[^\s<>"\']*images[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                    # åŒ…å«photoé—œéµå­—çš„URL
                    r'(https?://[^\s<>"\']*photo[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                    # åŒ…å«thumbé—œéµå­—çš„URL
                    r'(https?://[^\s<>"\']*thumb[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                    # åŒ…å«mediaé—œéµå­—çš„URL
                    r'(https?://[^\s<>"\']*media[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                    # åŒ…å«cdné—œéµå­—çš„URL
                    r'(https?://[^\s<>"\']*cdn[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                    # éå¸¸å¯¬é¬†çš„æ¨¡å¼ï¼šä»»ä½•çœ‹èµ·ä¾†åƒçœŸå¯¦ç¶²ç«™çš„åœ–ç‰‡URL
                    r'(https?://[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/[^\s<>"\']*\.(?:jpg|jpeg|png|webp|gif))',
                ]
                
                for pattern in loose_patterns:
                    matches = re.findall(pattern, html_content, re.IGNORECASE)
                    print(f"  ğŸ” å¯¬é¬†æ¨¡å¼æ‰¾åˆ° {len(matches)} å€‹åŒ¹é…")
                    for match in matches:
                        if match not in found_urls and len(found_urls) < per_page * 2:
                            found_urls.add(match)
                    
                    if len(found_urls) >= per_page:
                        break
                
                print(f"ğŸ” å¯¬é¬†æœå°‹å¾Œç¸½å…±æ‰¾åˆ° {len(found_urls)} å€‹URL")
            
            # è½‰æ›ç‚ºçµæœæ ¼å¼
            valid_count = 0
            skipped_count = 0
            
            # å°‡URLæŒ‰å„ªå…ˆç´šæ’åºï¼ˆå„ªå…ˆé¸æ“‡é«˜å“è³ªçš„URLï¼‰
            url_list = list(found_urls)
            priority_urls = []
            other_urls = []
            
            for url in url_list:
                # å„ªå…ˆé¸æ“‡ç›´æ¥çš„åœ–ç‰‡URLå’Œé«˜å“è³ªä¾†æº
                if (any(domain in url for domain in ['wikipedia.org', 'wikimedia.org', 'unsplash.com', 'pexels.com']) or
                    url.startswith('https://') and any(ext in url for ext in ['.jpg', '.jpeg', '.png', '.webp']) and
                    'encrypted-tbn' not in url):
                    priority_urls.append(url)
                else:
                    other_urls.append(url)
            
            # åˆä½µå„ªå…ˆç´šURLå’Œå…¶ä»–URL
            sorted_urls = priority_urls + other_urls
            
            print(f"ğŸ“Š URLå„ªå…ˆç´šæ’åº: {len(priority_urls)} å€‹é«˜å„ªå…ˆç´š, {len(other_urls)} å€‹ä¸€èˆ¬å„ªå…ˆç´š")
            
            for i, img_url in enumerate(sorted_urls[:per_page * 3]):  # æª¢æŸ¥æ›´å¤šURL
                # è·³éæ˜é¡¯ç„¡æ•ˆçš„URL
                skip_reasons = []
                
                if '/1x1_' in img_url or 'spacer.gif' in img_url:
                    skip_reasons.append("å°ºå¯¸éå°")
                elif img_url.startswith('data:image') and len(img_url) < 200:
                    skip_reasons.append("Base64åœ–ç‰‡å¤ªå°")
                elif 'logo' in img_url.lower() and any(size in img_url for size in ['16x16', '32x32', '48x48']):
                    skip_reasons.append("å°åœ–ç¤º")
                elif len(img_url) < 20:
                    skip_reasons.append("URLå¤ªçŸ­")
                
                if skip_reasons:
                    skipped_count += 1
                    if skipped_count <= 5:  # åªé¡¯ç¤ºå‰5å€‹è·³éçš„URL
                        print(f"â­ï¸ è·³éåœ–ç‰‡ ({', '.join(skip_reasons)}): {img_url[:80]}...")
                    continue
                
                print(f"âœ… æ·»åŠ åœ–ç‰‡ {valid_count + 1}: {img_url[:80]}...")
                    
                image_info = {
                    'id': f"google_scrape_{int(time.time())}_{valid_count}",
                    'url': img_url,
                    'thumb_url': img_url,
                    'title': f"{query} - åœ–ç‰‡ {valid_count + 1}",
                    'description': f"å¾Googleæœå°‹ç²å¾—çš„ '{query}' ç›¸é—œåœ–ç‰‡",
                    'width': None,
                    'height': None,
                    'source_url': '',
                    'file_type': self._get_file_extension(img_url).upper(),
                    'platform': 'google',
                    'attribution': 'ä¾†æº: Googleåœ–ç‰‡æœå°‹',
                    'download_url': img_url
                }
                results.append(image_info)
                valid_count += 1
                
                # é”åˆ°ç›®æ¨™æ•¸é‡å°±åœæ­¢
                if valid_count >= per_page:
                    break
            
            if skipped_count > 5:
                print(f"â­ï¸ å¦å¤–è·³éäº† {skipped_count - 5} å€‹ç„¡æ•ˆURL...")
            
            print(f"ğŸ“ˆ è™•ç†çµæœ: æœ‰æ•ˆåœ–ç‰‡ {valid_count} å¼µï¼Œè·³é {skipped_count} å€‹URL")
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°è¶³å¤ çš„çµæœï¼Œè£œå……ä¸€äº›ç›¸é—œçš„ç¤ºä¾‹
            if len(results) < 3:
                print("âš ï¸ æœå°‹çµæœè¼ƒå°‘ï¼Œè£œå……ç¤ºä¾‹åœ–ç‰‡")
                fallback_results = self._get_fallback_results(query, per_page - len(results))
                results.extend(fallback_results['results'])
            
            print(f"âœ… æœ€çµ‚è¿”å› {len(results)} å¼µåœ–ç‰‡")
            
            return {
                'success': True,
                'query': query,
                'page': page,
                'per_page': per_page,
                'total_results': len(results),
                'results': results,
                'images': results,  # ç‚ºå‰ç«¯å…¼å®¹æ€§æ·»åŠ 
                'platform': 'google',
                'mode': 'web_scraping',
                'message': f'æ‰¾åˆ° {len(results)} å¼µåœ–ç‰‡ (Web Scrapingæ¨¡å¼)'
            }
            
        except Exception as e:
            print(f"âŒ Web scrapingæœå°‹å¤±æ•—: {e}")
            # å¦‚æœscrapingå¤±æ•—ï¼Œæä¾›ä¸€äº›ç¤ºä¾‹çµæœ
            return self._get_fallback_results(query, per_page)
    
    def _get_file_extension(self, url: str) -> str:
        """å¾URLä¸­æå–æª”æ¡ˆå‰¯æª”å"""
        try:
            parsed = urlparse(url)
            path = parsed.path.lower()
            if '.jpg' in path:
                return 'jpg'
            elif '.jpeg' in path:
                return 'jpeg'
            elif '.png' in path:
                return 'png'
            elif '.webp' in path:
                return 'webp'
            elif '.gif' in path:
                return 'gif'
            else:
                return 'jpg'  # é è¨­
        except:
            return 'jpg'
    
    def _get_fallback_results(self, query: str, per_page: int) -> Dict:
        """æä¾›å‚™ç”¨ç¤ºä¾‹çµæœ"""
        results = []
        
        # ç‚ºä¸åŒé—œéµå­—æä¾›æ›´ç›¸é—œçš„ç¤ºä¾‹åœ–ç‰‡
        keyword_seeds = {
            'æµ·è±¡': [1001, 1002, 1003, 1004, 1005, 1006],
            'è²“': [200, 201, 202, 203, 204, 205],
            'ç‹—': [220, 221, 222, 223, 224, 225],
            'èŠ±': [100, 101, 102, 103, 104, 105],
            'å±±': [300, 301, 302, 303, 304, 305],
            'æµ·': [400, 401, 402, 403, 404, 405],
            'è»Š': [500, 501, 502, 503, 504, 505],
            'å»ºç¯‰': [600, 601, 602, 603, 604, 605],
            'é£Ÿç‰©': [700, 701, 702, 703, 704, 705],
            'äººç‰©': [800, 801, 802, 803, 804, 805]
        }
        
        # æ‰¾åˆ°æœ€ç›¸é—œçš„ç¨®å­
        seeds = None
        for keyword, seed_list in keyword_seeds.items():
            if keyword in query:
                seeds = seed_list
                break
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šé—œéµå­—ï¼Œä½¿ç”¨é€šç”¨ç¨®å­
        if not seeds:
            import hashlib
            hash_int = int(hashlib.md5(query.encode()).hexdigest()[:8], 16)
            seeds = [(hash_int + i) % 1000 for i in range(per_page)]
        
        for i in range(min(per_page, len(seeds))):
            seed = seeds[i]
            results.append({
                'id': f"demo_{query}_{i+1}",
                'url': f"https://picsum.photos/800/600?random={seed}",
                'thumb_url': f"https://picsum.photos/300/200?random={seed}",
                'title': f"ç¤ºä¾‹åœ–ç‰‡: {query} #{i+1}",
                'description': f"é€™æ˜¯é—œæ–¼ '{query}' çš„ç›¸é—œç¤ºä¾‹åœ–ç‰‡",
                'width': 800,
                'height': 600,
                'source_url': 'https://picsum.photos',
                'file_type': 'JPG',
                'platform': 'demo',
                'attribution': 'ç¤ºä¾‹åœ–ç‰‡ä¾†æº: Lorem Picsum',
                'download_url': f"https://picsum.photos/800/600?random={seed}"
            })
        
        return {
            'success': True,
            'query': query,
            'total_results': len(results),
            'results': results,
            'images': results,  # ç‚ºå‰ç«¯å…¼å®¹æ€§æ·»åŠ 
            'platform': 'demo',
            'mode': 'fallback',
            'message': f'ç¤ºä¾‹æ¨¡å¼ï¼šé¡¯ç¤º {len(results)} å¼µ "{query}" ç›¸é—œç¤ºä¾‹åœ–ç‰‡'
        }
    
    def download_image(self, image_url: str, filename: str = None) -> Dict:
        """
        ä¸‹è¼‰åœ–ç‰‡åˆ°æœ¬åœ°
        
        Args:
            image_url: åœ–ç‰‡URL
            filename: è‡ªå®šç¾©æª”æ¡ˆåç¨±
        """
        try:
            # ç”¢ç”Ÿæª”æ¡ˆåç¨±
            if not filename:
                file_id = str(uuid.uuid4())[:8]
                timestamp = int(time.time())
                # å¾URLä¸­æå–å‰¯æª”å
                parsed_url = urlparse(image_url)
                path = parsed_url.path
                if '.' in path:
                    extension = path.split('.')[-1].lower()
                    if extension not in ['jpg', 'jpeg', 'png', 'webp', 'gif']:
                        extension = 'jpg'
                else:
                    extension = 'jpg'
                filename = f"google_img_{timestamp}_{file_id}.{extension}"
            
            # è¨­å®šä¸‹è¼‰è·¯å¾‘
            download_path = self.download_dir / filename
            
            # ä¸‹è¼‰åœ–ç‰‡
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(image_url, headers=headers, timeout=120)
            response.raise_for_status()
            
            # æª¢æŸ¥å…§å®¹é¡å‹
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                return {
                    'success': False,
                    'error': f'ä¸æ˜¯æœ‰æ•ˆçš„åœ–ç‰‡æ ¼å¼: {content_type}',
                    'message': 'ä¸‹è¼‰çš„å…§å®¹ä¸æ˜¯åœ–ç‰‡æ ¼å¼'
                }
            
            # å„²å­˜æª”æ¡ˆ
            with open(download_path, 'wb') as f:
                f.write(response.content)
            
            # æª¢æŸ¥æª”æ¡ˆå¤§å°
            file_size = download_path.stat().st_size
            if file_size == 0:
                download_path.unlink()  # åˆªé™¤ç©ºæª”æ¡ˆ
                return {
                    'success': False,
                    'error': 'ä¸‹è¼‰çš„æª”æ¡ˆç‚ºç©º',
                    'message': 'åœ–ç‰‡ä¸‹è¼‰å¤±æ•—'
                }
            
            return {
                'success': True,
                'filename': filename,
                'file_path': str(download_path),
                'file_size': file_size,
                'download_url': f"/static/downloaded_images/{filename}",
                'message': f'åœ–ç‰‡å·²æˆåŠŸä¸‹è¼‰: {filename}'
            }
            
        except requests.exceptions.RequestException as e:
            return {
                'success': False,
                'error': f'ä¸‹è¼‰å¤±æ•—: {str(e)}',
                'message': 'ç„¡æ³•ä¸‹è¼‰åœ–ç‰‡ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š'
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'ä¸‹è¼‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}',
                'message': 'åœ–ç‰‡ä¸‹è¼‰å¤±æ•—'
            }
    
    def get_search_options(self) -> Dict:
        """ç²å–æœå°‹é¸é …"""
        return {
            'sizes': [
                {'id': 'any', 'name': 'ä»»ä½•å°ºå¯¸'},
                {'id': 'large', 'name': 'å¤§å°ºå¯¸'},
                {'id': 'medium', 'name': 'ä¸­ç­‰å°ºå¯¸'},
                {'id': 'icon', 'name': 'åœ–ç¤º'}
            ],
            'types': [
                {'id': 'photo', 'name': 'ç…§ç‰‡'},
                {'id': 'clipart', 'name': 'ç¾å·¥åœ–æ¡ˆ'},
                {'id': 'lineart', 'name': 'ç·šæ¢åœ–'},
                {'id': 'face', 'name': 'äººè‡‰'}
            ],
            'orientations': [
                {'id': 'any', 'name': 'ä»»ä½•æ–¹å‘'},
                {'id': 'landscape', 'name': 'æ©«å‘'},
                {'id': 'portrait', 'name': 'ç›´å‘'}
            ]
        } 